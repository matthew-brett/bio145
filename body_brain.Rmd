---
jupyter:
  jupytext_format_version: '1.0'
  jupytext_formats: Rmd:rmarkdown,ipynb
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
  language_info:
    codemirror_mode:
      name: ipython
      version: 3
    file_extension: .py
    mimetype: text/x-python
    name: python
    nbconvert_exporter: python
    pygments_lexer: ipython3
    version: 3.6.4
---

# Regression, body and brain

## About this page

This is a Jupyter Notebook.  It can be run as an interactive demo, or you can
read it as a web page.

You don't need to understand the code on this page, the text will tell you
what the code is doing.

You can also [run this demo
interactively](https://mybinder.org/v2/gh/matthew-brett/bio145/master?filepath=on_correlation.ipynb).

## The example problem

We are going to do regression of body weights and brain weights of some animals, and then look at the correlation.


## Some setup

We first need to get our environment set up to run the code and plots we
need.

```{python}
# Code to get set up.  If you are running interactively, you can execute
# this cell by pressing Shift and Enter at the same time.
# Libraries for arrays and plotting
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
# Make plots look a little bit more fancy
plt.style.use('fivethirtyeight')
# Import library for statistical routines
import scipy.stats as sps
# Print array numbers to 4 digits of precisiono
np.set_printoptions(precision=4, suppress=True)
```

## Starting with a line

Here are the body weights (in kilograms) from the 8 animals:

```{python}
body_weight = np.array([3.3, 465, 27.7, 521, 192, 2.5, 0.3, 55.5])
```

These are the corresponding brain weights (in grams):

```{python}
brain_weight = np.array([25.6, 423, 115, 655, 180, 12.1, 1.9, 175])
```

We believe that there is some relationship between `body_weight` and `brain_weight`.
Plotting them together we get:

```{python}
plt.plot(body_weight, brain_weight, '+')
plt.xlabel('Body')
plt.ylabel('Brain');
```

It looks like there may be some sort of straight line relationship. We could
try to find a good line to fit the data.  Here I will do some magic to work
out a good line.

```{python}
slope, intercept, r, p, se = sps.linregress(body_weight, brain_weight)
print(f'Slope: {slope:.4f}')
print(f'Intercept: {intercept:.4f}')
```

We also got the correlation *r* value from this calculation.  Here it is, for
future reference.  We will come back to this later:

```{python}
# Correlation "r" value
print(f'Correlation r: {r:.4f}')
```

This is the squared *r* value ($r^2$):

```{python}
r ** 2
```

Here is the line drawn on the plot of the data:

```{python}
# Plot data with the prediction
plt.plot(body_weight, brain_weight, 'k+')
mx = max(body_weight)
x_vals = [0, mx]
y_vals = [intercept, intercept + slope * mx]
plt.plot(x_vals, y_vals, 'b')
plt.xlabel('Body')
plt.ylabel('Brain')
plt.title('Body vs Brain with nice line');
```

## How do we chose a good line?


The line gives a *prediction* of what `brain_weight` should be, for any value
of `body_weight`.  If we have some value `x` for `body_weight`, then we can
predict the value `y` of `brain_weight`, with `y = intercept + slope * x`.

For example, here are the first values for `body_weight` and `brain_weight`:

```{python}
print(f'First body_weight value {body_weight[0]}')
print(f'First brain_weight value {brain_weight[0]}')
```

The second value is the *actual* value of `brain_weight`.  The *predicted*
value of `brain_weight`, for this value of `body_weight` is:

```{python}
predicted = intercept + body_weight[0]
predicted
```

The *error* for our line, is the difference between the actual and predicted
value.

```{python}
actual = brain_weight[0]
error = actual - predicted
error
```

This is the error for the first value.  We can get the errors for all the
values in the same way.


This is the calculation of error for all 12 values.  As usual, you don't need
to understand the code in detail:

```{python}
all_predicted = intercept + body_weight * slope
all_errors = brain_weight - all_predicted
all_errors
```

Notice the first value for `all_errors` is the same as the value for `error`
we saw above.

The errors here are the distances between the prediction line and the points
on the plot.  Here I show the errors as red lines. Don't worry about the code
below, it's not important to the idea.

```{python}
# Plot data with the prediction and errors
plt.plot(body_weight, brain_weight, 'k+', ms=15)
mx = max(body_weight)
x_vals = [0, mx]
y_vals = [intercept, intercept + slope * mx]
plt.plot(x_vals, y_vals, 'b')
# Draw the error lines
for i in range(len(body_weight)):
    x_vals = [body_weight[i], body_weight[i]]
    y_vals = [all_predicted[i], brain_weight[i]]
    plt.plot(x_vals, y_vals, 'r')
plt.xlabel('Body weight')
plt.ylabel('Brain weight')
plt.title('body_weight vs brain_weight, and errors');
```

A good line will make the errors as small as possible.  Therefore, a good line
will make the lengths of the red lines as short as possible.

We need to generate a single number, from the errors, that gives an overall
measure of the size of the errors.

We cannot just add up the errors, because the negative and positive errors
will cancel out.  Even if the errors are a mixture of large positive and large
negative, the sum could be very small.

The usual thing to do, is to square all the errors, to make sure they are all
positive.  Then we add all the squared errors.  This gives the *sum of squared
error* or SSE.

```{python}
# A reminder of the errors we calculated above
all_errors
```

```{python}
# Square all the errors
squared_errors = all_errors ** 2
squared_errors
```

```{python}
# Calculate the sum of the squared errors
SSE = sum(squared_errors)
SSE
```

The line is a good one when SSE is small.  In fact, the usual "best fit" line
chosen by packages such as Excel, is the line that gives the lowest SSE value,
of all possible lines.

It is the line that minimizes the squared error, often called the *least squares* line.

This is the line that I found by sort-of magic, above.  If you like, try other
slopes and intercepts.  You will find that they always have a higher SSE value
than the slope and intercept I have used here.


## Regression and correlation

Above, you have seen regression, using the *least squares* line.

Correlation is a version of the same thing, but where we have *standardized*
the data.

We standardize data by subtracting the mean, and dividing by the standard
deviation.

We do this, to put the x and y values onto the same scale.

For example, here is a histogram of the `body_weight` values, to give you an idea
of their position and spread.

```{python}
plt.hist(body_weight)
plt.title("Body weight values");
```

In correlation, we are interested to know whether the *variation* in the (e.g)
`body_weight` values, predicts the variation in the (e.g)  `brain_weight` values.
Variation, is variation around the mean.  To show variation, we subtract the
mean.  We refer to the values, with the mean subtracted, as *mean centered*.

```{python}
centered_x = body_weight - body_weight.mean()
plt.hist(centered_x)
plt.title('Mean-centered body weight values');
```

Finally, the values for the spread either side of zero depends on the units of
the measurement.   We measure the spread, with standard deviation:

```{python}
std_x = np.std(centered_x)
std_x
```

We would like to re-express our data to have a standard spread, that is
comparable for the `x` / `body_weight` values and the `y` / `brain_weight` values.
For example, we might like to ensure the data have a standard deviation of 1.
To do this, we divide the centered values by the standard deviation.

```{python}
standard_x = centered_x / std_x
plt.hist(standard_x)
plt.title('Standardized body weight values');
```

You will see below, that the mean of these values is now 0, and the standard deviation is 1.

```{python}
print(f'Mean of standard x: {np.mean(standard_x):.4f}')
print(f'Standard deviation: {np.std(standard_x):.4f}')
```

Our `body_weight` values are now *standardized*.

We do the same for our `y` / `brain_weight` values:

```{python}
# Standarize the y / brain_weight values
centered_y = brain_weight - brain_weight.mean()
standard_y = centered_y / np.std(centered_y)
print(f'Mean of standard y: {np.mean(standard_y):.4f}')
print(f'Standard deviation: {np.std(standard_y):.4f}')
```

The correlation value *r* is just the slope of the regression line relating
our standardized `x` / `body_weight` and standardized `y` / `brain_weight`:

```{python}
std_slope, std_intercept, std_r, p, se = sps.linregress(standard_x, standard_y)
print(f'Standardized slope (=correlation r): {std_slope:.4f}')
print(f'Standardized intercept: {std_intercept:.4f}')
```

It turns out that, when we standardize the x and y values, as we did here, the
*intercept* for the least-squares line must be zero, for mathematical reasons
that are not important for our current purpose.

Notice that the slope above is the same as the `r` value for the original
regression line:

```{python}
print(f'Standardized slope: {std_slope:.4f}')
print(f'Original r for regression: {r:.4f}')
```

Here is the plot of standardized `body_weight` against standardized `brain_weight`,
with the least-squares line:

```{python}
# Plot standard data with the prediction
plt.plot(standard_x, standard_y, '+')
mx = max(standard_x)
mn = min(standard_x)
x_vals = [mn, mx]
y_vals = [std_intercept + std_slope * mn, std_intercept + std_slope * mx]
plt.plot(x_vals, y_vals, 'b')
plt.title('Standardized body weight against standardized brain weight');
```

Notice that the plot has the point (0, 0) at its center, and that the line
goes through the (0, 0) point.  The slope of the line, is the correlation
value *r*.

It turns out that, if we do this standardization procedure, the slope of the
line can only vary between 1 (where the standardized `x` values are the same as
the standardized `y` values) and -1 (where the standardized `x` values are the
exact negative of the standardized `y` values).
